# Chapter 4: Clustering and classification

Loading data:
```{r}
# access the MASS package
library(MASS)

# load the data
data("Boston")
```

Let's explore the data
```{r}
library(Hmisc)
library(dbplyr)
library(tidyverse)
library(ggplot2)
library(corrplot)
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round()

# print the correlation matrix


# visualize the correlation matrix
corrplot(cor_matrix, method="circle",type = "upper",cl.pos = "b",tl.pos = "d", tl.cex = 0.6)
pairs(Boston[1:7])
pairs(Boston[7:14])
pairs(Boston)
#hist.data.frame(Boston)
summary(Boston)
```

From the correlation plot we see, that the dummy variable (chas) and does not have any apparent connection to any other variable. Also, the variable explaning the proportions of blacks (black) seems to be not in any connection with other variable. From the matrix plot, there is an interesting relationship between the lower status of the population (lstat) and median value of owner-occupied homes (medv), which seems to be logarithimc, or exponential. In some variables we can see, that there is some clear clustering in two groups (rad and tax vs most of the other variables)

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)


# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low","med_low","med_high","high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

We have scaled our variables in the Boston dataset and we have created two new variables: train and test, in train there is 80% of the dataset. Next, we will perform the linear discriminant analysis on the train set.

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2,col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2.5)
```

```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

Our model has predicted the classes pretty nicely. Biggest differences were in the crimes from the 'medium high' were predicted as 'medium low'. Out of 102 tests, 28 were predicted incorrectly. Nevetherless, those predicted incorrectly were misplaced only by one class, which in this case should not create big differences.

```{r}
# re-loading the data
data("Boston")

# center and standardize variables
boston_scaled <- scale(Boston)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
# euclidean distance matrix
dist_eu <- dist(Boston)

# look at the summary of the distances
summary(dist_eu)

# k-means clustering
km <-kmeans(Boston, centers = 3)

# plot the Boston dataset with clusters
pairs(Boston[6:10], col = km$cluster)
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston[6:10], col = km$cluster)
```

From the  within cluster sum of squares (WCSS) plot, we see that the line has drastically dropped after the value 2, so we have chosen 2 cluster for our k-means analysis.